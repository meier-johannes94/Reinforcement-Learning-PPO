{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'episode_max_steps': 402, 'filename': 'Lbda=0.95,Gamma=0.95', 'save_episodes': 14, 'K_epochs': 10, 'eps_clip': 0.25, 'policy_depth': 2, 'policy_width': 128, 'value_depth': 4, 'value_width': 256, 'activation_function': <ActivationFunctions.TANH: 1>, 'minimum_std': 0.01, 'initial_std': 0.5, 'policy_last_layer_scaler': 0.01, 'value_last_layer_scaler': 0.001, 'initializer': <Initializers.ORTHOGONAL: 3>, 'lbda': 0.95, 'mini_batch_size': 128, 'batch_mode': <BatchModes.SHUFFLE_RECOMPUTE_ADVANTAGES: 2>, 'update_timesteps': 20000, 'gamma': 0.95, 'handle_abandoned': True, 'frame_skipping_length': 1, 'optimizer_lr': 0.0003, 'optimizer_weight_decay': 0.0, 'optimizer_momentum': 0.9, 'optimizer_epsilon': 1e-08, 'advantage_normalization': False, 'reward_normalization': False, 'input_normalization': True, 'gradient_clipping': 2.0, 'input_clipping_max_abs_value': 10.0, 'weighting_true_reward': 1, 'weighting_reward_winner': 0, 'weighting_reward_closeness_puck': 0, 'weighting_reward_touch_puck': 0, 'weighting_reward_puck_direction': 0, 'default_timestep_loss': 0.0} \n",
      "\n",
      "14: ## Learn(R;W,D,L in %): -4.8, 25, 125, 100 Eval(R;W,D,L in %): -1.4, 25, 50, 25  - Checkpoint saved\n",
      "28: ## Learn(R;W,D,L in %): -2.7, 50, 125, 75 Eval(R;W,D,L in %): -2.2, 25, 50, 25 \n",
      "42: ## Learn(R;W,D,L in %): -4.4, 25, 125, 100 Eval(R;W,D,L in %): -5.6, 0, 50, 50 \n",
      "56: ## Learn(R;W,D,L in %): -6.7, 0, 175, 75 Eval(R;W,D,L in %): -0.5, 25, 75, 0  - Checkpoint saved\n",
      "70: ## Learn(R;W,D,L in %): -4.6, 25, 175, 50 Eval(R;W,D,L in %): -8.8, 0, 50, 50 \n",
      "84: ## Learn(R;W,D,L in %): -4.5, 50, 125, 75 Eval(R;W,D,L in %): -7.3, 0, 100, 0 \n",
      "98: ## Learn(R;W,D,L in %): -1.9, 50, 175, 25 Eval(R;W,D,L in %): -4.4, 0, 100, 0 \n",
      "112: ## Learn(R;W,D,L in %): -4.2, 25, 150, 75 Eval(R;W,D,L in %): -15.7, 0, 50, 50 \n",
      "126: ## Learn(R;W,D,L in %): -8.0, 25, 125, 100 Eval(R;W,D,L in %): -8.0, 0, 50, 50 \n",
      "140: ## Learn(R;W,D,L in %): -6.0, 25, 100, 125 Eval(R;W,D,L in %): 0.4, 25, 75, 0  - Checkpoint saved\n",
      "154: ## Learn(R;W,D,L in %): -6.4, 50, 50, 150 Eval(R;W,D,L in %): -1.6, 25, 50, 25 \n",
      "168: ## Learn(R;W,D,L in %): 0.3, 100, 100, 50 Eval(R;W,D,L in %): 3.7, 50, 50, 0  - Checkpoint saved\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ac489348b578>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Lbda=0.95,Gamma=0.95\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m train(training_episodes, \n\u001b[0m\u001b[0;32m     21\u001b[0m       \u001b[0mtraining_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m       \u001b[0meval_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Johannes\\Desktop\\Modify\\Reinforcement-Learning-PPO\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(training_episodes, training_length, eval_length, save_episodes, episode_max_steps, gamma, K_epochs, eps_clip, policy_depth, policy_width, value_depth, value_width, activation_function, initializer, policy_last_layer_scaler, value_last_layer_scaler, minimum_std, initial_std, handle_abandoned, reward_normalization, mini_batch_size, batch_mode, optimizer_lr, optimizer_weight_decay, optimizer_momentum, optimizer_epsilon, frame_skipping_length, advantage_normalization, input_normalization, update_timesteps, opponent_type, opponent_weak, default_timestep_loss, frame_skip_frequency, input_clipping_max_abs_value, gradient_clipping, lbda, filename, seed, load_filename, print_config, load_info)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;31m# Run one episode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_max_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m             \u001b[0ma2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayer2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mHiddenPrints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Johannes\\Desktop\\Modify\\Reinforcement-Learning-PPO\\Agent.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    355\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"frame_skipping_length\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m                     if self._current_frame_skip_activated else 1)\n\u001b[1;32m--> 357\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_memory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_frame_skip_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Johannes\\Desktop\\Modify\\Reinforcement-Learning-PPO\\PPO.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, memory)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Johannes\\Desktop\\Modify\\Reinforcement-Learning-PPO\\PPO.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, memory)\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[1;31m# Save taken action, states, log probabilities for model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[0maction_logprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\distributions\\multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_batch_mahalanobis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mhalf_log_det\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mhalf_log_det\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train import train\n",
    "from PPO import ActivationFunctions, Initializers, BatchModes\n",
    "from Agent import OpponentType\n",
    "\n",
    "#Non default hyperparameters\n",
    "lbda = 0.95\n",
    "gamma = 0.95\n",
    "\n",
    "#Training configuration\n",
    "training_length = 10\n",
    "eval_length = 4\n",
    "episode_max_steps = 402\n",
    "save_episodes = training_length + eval_length\n",
    "opponent_type = OpponentType.NORMAL\n",
    "opponent_weak = False\n",
    "training_episodes = 40000\n",
    "seed = 123456\n",
    "filename = \"lbda=0.95,gamma=0.95\"\n",
    "\n",
    "train(training_episodes, \n",
    "      training_length, \n",
    "      eval_length,\n",
    "      save_episodes, \n",
    "      gamma = gamma , \n",
    "      lbda = lbda,\n",
    "      filename = filename, \n",
    "      seed = seed, \n",
    "      print_config=True, \n",
    "      load_info=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "from test_agent import test_agent\n",
    "\n",
    "print(\"Playing against normal opponent: \\n\")\n",
    "test_agent(30, filename, True)\n",
    "\n",
    "print(\"\\n\\n\\nSelf-Play: \\n\")\n",
    "test_agent(30, filename, True, checkpoint_selfplay_filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.3 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "951b67a811c67cbb407e4a19bf29e038a158d4ed31c2fea63332fa350584f881"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}